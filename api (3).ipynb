{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6f3cf5-9484-4ffb-a562-8c9def2e953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import json, os, time\n",
    "import requests\n",
    "import random\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_START = datetime(2026, 2, 4)\n",
    "DATA_END = datetime(2026, 2, 10)\n",
    "DUMMY_PATH = 'dummy.json'\n",
    "REAL_URL = \"https://dummyjson.com/products\"\n",
    "KEY_FILE = 'key.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d177d97-0cc9-4b3e-8689-0e015ceeb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_time_window(start, end, window_days: int = 2):\n",
    "    \"\"\"\n",
    "    Generate a random time window between of fixed length (window_days) between 'start' and 'end'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start : The start of the overall time range.\n",
    "    end : The end of the overall time range.\n",
    "    window_days : The window of time the start and end range is.\n",
    "\n",
    "    \"\"\"\n",
    "    total_seconds = int((end - start).total_seconds())\n",
    "    window_seconds = window_days * 86400 # convert days to seconds\n",
    "    \n",
    "    if total_seconds < window_seconds:\n",
    "        raise ValueError(f\"Time range must be at least {windows_days} days long.\")\n",
    "\n",
    "    # pick random start point\n",
    "    rand_start_seconds = random.randint(0, total_seconds - window_seconds)\n",
    "    rand_start = start + timedelta(seconds=rand_start_seconds)\n",
    "    rand_end = rand_start + timedelta(days=window_days)\n",
    "\n",
    "    return (\n",
    "        rand_start.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        rand_end.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    )\n",
    "\n",
    "def random_comment_window(tweet_created_at, window_days: int = 2):\n",
    "    \"\"\"\n",
    "    Create a random comment window starting after the tweet timestamp.\n",
    "    \"\"\"\n",
    "    tweet_time = datetime.fromisoformat(tweet_created_at.replace(\"Z\", \"+00:00\"))\n",
    "    now = datetime.now(timezone.utc) - timedelta(seconds=15) # offsetting the time for acceptable api timing\n",
    "\n",
    "    # max time you’re allowed to search comments\n",
    "    max_end = min(tweet_time + timedelta(days=window_days), now)\n",
    "\n",
    "    if max_end <= tweet_time:\n",
    "        raise ValueError(\"Tweet is in the future or no valid comment window.\")\n",
    "\n",
    "    # pick a random start\n",
    "    delta_seconds = int((max_end - tweet_time).total_seconds() - window_days * 86400)\n",
    "    delta_seconds = max(delta_seconds, 0)\n",
    "    \n",
    "    rand_start_seconds = random.randint(0, delta_seconds) if delta_seconds > 0 else 0\n",
    "    rand_start = tweet_time + timedelta(seconds=rand_start_seconds)\n",
    "\n",
    "    rand_end = min(rand_start + timedelta(days = window_days), max_end)\n",
    "\n",
    "    return (\n",
    "        rand_start.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        rand_end.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    )\n",
    "    \n",
    "def save_json(data: dict, folder: str, filename: str):\n",
    "    \"\"\"\n",
    "    Function to save dictionaries as jsons\n",
    "    \"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    path = os.path.join(folder, filename)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Saved → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5550bf-0df7-4cbf-a2c3-622a534fc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfinished Dummy API code\n",
    "\n",
    "class BaseAPIClient(ABC):\n",
    "    '''\n",
    "    We only use this class to define a structure\n",
    "    and then inherit it. Never use this class directly.\n",
    "    '''\n",
    "\n",
    "    @abstractmethod # allows you to define a function with no purpose\n",
    "    def fetch_data(self) -> dict:\n",
    "        pass # doesnt do anything\n",
    "        \n",
    "class DummyAPIClient(BaseAPIClient):\n",
    "    '''\n",
    "    Since this inherist the BaseAPIClient it must\n",
    "    implement all of its abstract methods\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        print('running in dummy mode')\n",
    "        return\n",
    "\n",
    "    def fetch_data(self) -> dict:\n",
    "        '''\n",
    "        Load and return the local dummy data\n",
    "        '''\n",
    "        with open(DUMMY_PATH, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "# START of script\n",
    "\n",
    "class RealAPIClient(BaseAPIClient):\n",
    "    '''\n",
    "    Since this inherist the BaseAPIClient it must\n",
    "    implement all of its abstract methods\n",
    "    '''\n",
    "    def __init__(self, key, bearer):\n",
    "        self.key = key\n",
    "        self.bearer = bearer\n",
    "        return\n",
    "        \n",
    "    def fetch_data(self) -> dict:\n",
    "        \"\"\"Placeholder to satisfy abstract method requirement.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    def fetch_posts(self,\n",
    "                    politician_handle: str,\n",
    "                    randomize_time=True,\n",
    "                    target_count=10\n",
    "                   ):\n",
    "        politician_folder = f\"data/posts/{politician_handle}\"\n",
    "        os.makedirs(politician_folder, exist_ok=True)\n",
    "\n",
    "        posts_path = f\"{politician_folder}/{politician_handle}_posts.json\"\n",
    "\n",
    "        existing_posts = []\n",
    "        collected_post_ids = set()\n",
    "\n",
    "        # Load posts into exisitng_posts if they exist\n",
    "        if os.path.exists(posts_path):\n",
    "            with open(posts_path, \"r\") as f:\n",
    "                existing_posts = json.load(f).get(\"data\", [])\n",
    "                collected_post_ids = {tweet[\"id\"] for tweet in existing_posts}\n",
    "\n",
    "        # if we have enough posts, STOP\n",
    "        if len(existing_posts) >= target_count:\n",
    "            print(f\"Already have {len(existing_posts)} posts for {politician_handle}\")\n",
    "            return {\"data\": existing_posts[:target_count]}, [p[\"id\"] for p in existing_posts[:target_count]]\n",
    "\n",
    "        collected_posts = existing_posts[:]\n",
    "\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.bearer}\"}\n",
    "        \n",
    "        attempts = 0\n",
    "        while len(collected_posts) < target_count and attempts <5: # ensures that we only collect 10 posts in each folder\n",
    "            attempts += 1\n",
    "            added_this_round = 0 # number of posts added to the folder\n",
    "            # Step 1: Set or randomize timeframe\n",
    "            if randomize_time:\n",
    "                start, end = random_time_window(DATA_START, DATA_END)\n",
    "            else:\n",
    "                # manually set date window here\n",
    "                start = \"2025-12-16T19:00:00Z\"\n",
    "                end   = \"2025-12-16T21:00:00Z\"\n",
    "\n",
    "            print(f\"Fetching tweets for {politician_handle} between {start} and {end}\")\n",
    "\n",
    "            params = {\n",
    "                \"query\": f\"from:{politician_handle} -is:retweet\", # no retweets!\n",
    "                \"start_time\": start,\n",
    "                \"end_time\": end,\n",
    "                \"max_results\": 10,\n",
    "                \"tweet.fields\": \"id,text,created_at\"\n",
    "            }\n",
    "    \n",
    "            # Get response\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(\"SUCCESS\")\n",
    "                data = response.json().get(\"data\", [])\n",
    "                for tweet in data:\n",
    "                    pid = tweet[\"id\"]\n",
    "                    if pid not in collected_post_ids:\n",
    "                        #print(f\"Skipping post {pid} — already saved\")\n",
    "                        collected_posts.append(tweet)\n",
    "                        collected_post_ids.add(pid)\n",
    "                        added_this_round += 1\n",
    "                    # only collect 10 posts in the foslder\n",
    "                    if len(collected_posts) >= target_count:\n",
    "                        break\n",
    "                if added_this_round == 0:\n",
    "                    print(\"No new posts found - stopping early\")\n",
    "                    continue\n",
    "                        \n",
    "            elif response.status_code == 429:\n",
    "                reset = response.headers.get(\"x-rate-limit-reset\")\n",
    "                wait = int(reset) - int(time.time()) if reset else 60\n",
    "                print(f\"→ RATE LIMIT HIT. Sleeping {wait} seconds…\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(\"Error:\", response.text)\n",
    "                response.raise_for_status()\n",
    "\n",
    "        # SAVE FINAL POSTS\n",
    "        if collected_posts:\n",
    "            save_json(\n",
    "                {\"data\": collected_posts[:target_count]},\n",
    "                folder=politician_folder,\n",
    "                filename=f\"{politician_handle}_posts.json\"\n",
    "            )\n",
    "        return {\"data\": collected_posts[:target_count]}, [p[\"id\"] for p in collected_posts[:target_count]]\n",
    "\n",
    "    def fetch_comments(self,\n",
    "                       politician_handle: str,\n",
    "                       post_id: str,\n",
    "                       tweet_created_at: str,\n",
    "                       target_count=10,\n",
    "                       randomize_time: bool = True\n",
    "                      ):\n",
    "        post_folder = f\"data/posts/{politician_handle}/{post_id}\"\n",
    "        os.makedirs(post_folder, exist_ok=True)\n",
    "\n",
    "        comments_path = os.path.join(post_folder, \"comments.json\")\n",
    "        \n",
    "        existing_comments = []\n",
    "        existing_comment_ids = set()\n",
    "    \n",
    "        # LOAD EXISTING COMMENTS\n",
    "        if os.path.exists(comments_path):\n",
    "            with open(comments_path, \"r\") as f:\n",
    "                existing_comments = json.load(f).get(\"data\", [])\n",
    "                existing_comment_ids = {c[\"id\"] for c in existing_comments}\n",
    "        else:\n",
    "            existing_comments = []\n",
    "    \n",
    "        if len(existing_comments) >= target_count:\n",
    "            print(f\"Already have {len(existing_comments)} comments for {post_id}\")\n",
    "            return existing_comments[:target_count]\n",
    "    \n",
    "        collected_comments = existing_comments[:]\n",
    "\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.bearer}\"}\n",
    "\n",
    "        attempts = 0\n",
    "        while len(collected_comments) < target_count and attempts < 10:\n",
    "            attempts += 1\n",
    "            added_this_round = 0\n",
    "            \n",
    "            # Define Timeframe\n",
    "            start, end = random_comment_window(tweet_created_at, window_days=2)\n",
    "            print(f\"Fetching comments for {post_id} between {start} and {end}\")\n",
    "            \n",
    "            params = {\"query\": f\"conversation_id:{post_id}\",\n",
    "                      \"start_time\": start,\n",
    "                      \"end_time\": end,\n",
    "                      \"max_results\": 10,\n",
    "                     \"tweet.fields\": \"id,text,created_at\"}\n",
    "    \n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(\"SUCCESS\")\n",
    "                data = response.json().get(\"data\", [])\n",
    "\n",
    "                for tweet in data:\n",
    "                    cid = tweet['id']\n",
    "                    if cid not in existing_comment_ids:\n",
    "                        collected_comments.append(tweet)\n",
    "                        existing_comment_ids.add(cid)\n",
    "                        added_this_round += 1\n",
    "\n",
    "                    if len(collected_comments) >= target_count:\n",
    "                        break\n",
    "                if added_this_round == 0:\n",
    "                    print(\"No new comments found - stopping early\")\n",
    "                    break\n",
    "                \n",
    "            elif response.status_code == 429:\n",
    "                reset = response.headers.get(\"x-rate-limit-reset\")\n",
    "                wait = int(reset) - int(time.time()) if reset else 60\n",
    "                print(f\"→ RATE LIMIT HIT. Sleeping {wait} seconds…\")\n",
    "                time.sleep(wait)\n",
    "            else: #other Errors\n",
    "                print(\"→ ERROR:\", response.text)\n",
    "                response.raise_for_status()\n",
    "\n",
    "        save_json({\"data\": collected_comments[:target_count]},\n",
    "                  folder=post_folder,\n",
    "                  filename=\"comments.json\")\n",
    "\n",
    "        return collected_comments[:target_count]\n",
    "\n",
    "class ToxicityApp:\n",
    "    def __init__(self):\n",
    "        if self.load_key():\n",
    "            self.api = RealAPIClient(self.key, self.bearer)\n",
    "        else:\n",
    "            self.api = DummyAPIClient()\n",
    "\n",
    "    def load_key(self):\n",
    "        try:\n",
    "            with open(\"key.json\") as f:\n",
    "                data = json.load(f)\n",
    "                self.key = data[\"X_API_KEY\"]\n",
    "                self.bearer = data[\"bearer_token\"]\n",
    "            return True\n",
    "        except:\n",
    "            print(\"Could not load key.\")\n",
    "            return False\n",
    "\n",
    "    def collect_politician_comments(self, \n",
    "                                    handles: list[str], \n",
    "                                    randomize_time: bool = True\n",
    "                                   ):\n",
    "        \"\"\"\n",
    "        Handes a list of twitter handles, returns json of posts and comments\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for handle in handles:\n",
    "            print(f\"Collecting posts for {handle}\")\n",
    "            \n",
    "            posts_json, post_ids = self.api.fetch_posts(handle)\n",
    "            tweets = posts_json.get(\"data\", [])\n",
    "\n",
    "            if not post_ids:\n",
    "                print(f\"No posts found for {handle}.\")\n",
    "                results[handle] = {\"posts\": posts_json, \"comments\": {}}\n",
    "                continue\n",
    "                \n",
    "            comments_for_this_handle = {}\n",
    "\n",
    "            # Iterate posts\n",
    "            for tweet in tweets:\n",
    "                post_id = tweet[\"id\"]\n",
    "                created_at = tweet[\"created_at\"]\n",
    "\n",
    "                comment_data = self.api.fetch_comments(\n",
    "                    politician_handle=handle,\n",
    "                    post_id=post_id,\n",
    "                    tweet_created_at = created_at,\n",
    "                    target_count=10,\n",
    "                    randomize_time=randomize_time)\n",
    "                comments_for_this_handle[post_id] = comment_data\n",
    "        \n",
    "            # Store inside the results dictionary\n",
    "            results[handle] = {\n",
    "                \"posts\" : posts_json,\n",
    "                \"comments\": comments_for_this_handle\n",
    "            }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d408168-63f4-4063-86c5-745f015e3e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts for SenTuberville\n",
      "Already have 10 posts for SenTuberville\n",
      "Already have 10 comments for 2017728095743258826\n",
      "Already have 10 comments for 2017694628565135366\n",
      "Already have 10 comments for 2017652854744297983\n",
      "Already have 10 comments for 2017386285246169465\n",
      "Already have 10 comments for 2017317137682206839\n",
      "Already have 10 comments for 2017316916092662166\n",
      "Already have 10 comments for 2017279766496190651\n",
      "Already have 10 comments for 2017232811292262430\n",
      "Already have 10 comments for 2016987700696428746\n",
      "Already have 10 comments for 2016956008489275700\n",
      "Collecting posts for SenKatieBritt\n",
      "Already have 10 posts for SenKatieBritt\n",
      "Already have 10 comments for 2017015700963930351\n",
      "Already have 10 comments for 2016923809404633530\n",
      "Already have 10 comments for 2016647154593329514\n",
      "Already have 10 comments for 2016596831652712774\n",
      "Already have 10 comments for 2016556770018119692\n",
      "Already have 10 comments for 2016289208462111120\n",
      "Already have 10 comments for 2017385652195659929\n",
      "Already have 10 comments for 2017267599441117469\n",
      "Already have 10 comments for 2017260766177460498\n",
      "Already have 10 comments for 2017231172867100725\n",
      "Collecting posts for lisamurkowski\n",
      "Fetching tweets for lisamurkowski between 2026-02-06T19:02:05Z and 2026-02-08T19:02:05Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for lisamurkowski between 2026-02-06T19:59:47Z and 2026-02-08T19:59:47Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for lisamurkowski between 2026-02-05T02:21:20Z and 2026-02-07T02:21:20Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for lisamurkowski between 2026-02-04T08:14:21Z and 2026-02-06T08:14:21Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for lisamurkowski between 2026-02-07T09:47:50Z and 2026-02-09T09:47:50Z\n",
      "SUCCESS\n",
      "Saved → data/posts/lisamurkowski\\lisamurkowski_posts.json\n",
      "Already have 10 comments for 2017401976233853031\n",
      "Already have 10 comments for 2019160684060147917\n",
      "Fetching comments for 2020666841027219849 between 2026-02-09T01:11:31Z and 2026-02-10T19:48:13Z\n",
      "SUCCESS\n",
      "Saved → data/posts/lisamurkowski/2020666841027219849\\comments.json\n",
      "Collecting posts for SenDanSullivan\n",
      "Already have 10 posts for SenDanSullivan\n",
      "Already have 10 comments for 2018775866860359765\n",
      "Already have 10 comments for 2018489110604714306\n",
      "Already have 10 comments for 2018352666817228861\n",
      "Already have 10 comments for 2018077416544231789\n",
      "Already have 10 comments for 2017407877976056087\n",
      "Already have 10 comments for 2016991098141327471\n",
      "Fetching comments for 2019148194404610089 between 2026-02-04T20:36:58Z and 2026-02-06T20:36:58Z\n",
      "SUCCESS\n",
      "No new comments found - stopping early\n",
      "Saved → data/posts/SenDanSullivan/2019148194404610089\\comments.json\n",
      "Already have 10 comments for 2019169796726911357\n",
      "Already have 10 comments for 2019850836147617937\n",
      "Already have 10 comments for 2019558294030020674\n",
      "Collecting posts for SenRubenGallego\n",
      "Already have 10 posts for SenRubenGallego\n",
      "Already have 10 comments for 2018435577364783580\n",
      "Already have 10 comments for 2018434108867637523\n",
      "Already have 10 comments for 2017641816489697512\n",
      "Already have 10 comments for 2017413318026117338\n",
      "Already have 10 comments for 2017368734483018118\n",
      "Already have 10 comments for 2017342855048482987\n",
      "Already have 10 comments for 2017274867419107651\n",
      "Already have 10 comments for 2018821716240196082\n",
      "Already have 10 comments for 2018763554132783573\n",
      "Already have 10 comments for 2018748642107199757\n",
      "Collecting posts for SenMarkKelly\n",
      "Already have 10 posts for SenMarkKelly\n",
      "Already have 10 comments for 2018384059890602197\n",
      "Already have 10 comments for 2018014302989345240\n",
      "Already have 10 comments for 2019186959810433143\n",
      "Already have 10 comments for 2019174525653430579\n",
      "Already have 10 comments for 2019135374640001467\n",
      "Already have 10 comments for 2019121781886579112\n",
      "Already have 10 comments for 2019091415616344168\n",
      "Already have 10 comments for 2018830412265713765\n",
      "Already have 10 comments for 2018788515585393003\n",
      "Already have 10 comments for 2018763827454546368\n",
      "Collecting posts for JohnBoozman\n",
      "Already have 10 posts for JohnBoozman\n",
      "Already have 10 comments for 2018701002292023794\n",
      "Already have 10 comments for 2018684330302722514\n",
      "Already have 10 comments for 2018445022379851990\n",
      "Already have 10 comments for 2018349083560218665\n",
      "Already have 10 comments for 2018328894915457167\n",
      "Already have 10 comments for 2018122491688858026\n",
      "Already have 10 comments for 2017740788013105607\n",
      "Already have 10 comments for 2017717714912219646\n",
      "Already have 10 comments for 2018749849781338573\n",
      "Fetching comments for 2018820179296387241 between 2026-02-03T22:53:33Z and 2026-02-05T22:53:33Z\n",
      "SUCCESS\n",
      "No new comments found - stopping early\n",
      "Saved → data/posts/JohnBoozman/2018820179296387241\\comments.json\n",
      "Collecting posts for SenTomCotton\n",
      "Already have 10 posts for SenTomCotton\n",
      "Already have 10 comments for 2018124809524453800\n",
      "Already have 10 comments for 2017719795806798185\n",
      "Already have 10 comments for 2017668826628808993\n",
      "Already have 10 comments for 2017284474552176829\n",
      "Already have 10 comments for 2017269282954989836\n",
      "Already have 10 comments for 2017258622686757299\n",
      "Already have 10 comments for 2019102413458927869\n",
      "Already have 10 comments for 2019096404287762784\n",
      "Already have 10 comments for 2019091487263256800\n",
      "Already have 10 comments for 2019055912992788524\n",
      "Collecting posts for AlexPadilla4CA\n",
      "Fetching tweets for AlexPadilla4CA between 2026-02-05T03:04:01Z and 2026-02-07T03:04:01Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AlexPadilla4CA between 2026-02-05T22:06:44Z and 2026-02-07T22:06:44Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AlexPadilla4CA between 2026-02-07T04:22:27Z and 2026-02-09T04:22:27Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AlexPadilla4CA between 2026-02-05T03:26:48Z and 2026-02-07T03:26:48Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AlexPadilla4CA between 2026-02-06T22:27:45Z and 2026-02-08T22:27:45Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "No posts found for AlexPadilla4CA.\n",
      "Collecting posts for AdamSchiff\n",
      "Fetching tweets for AdamSchiff between 2026-02-07T04:04:47Z and 2026-02-09T04:04:47Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AdamSchiff between 2026-02-07T01:14:37Z and 2026-02-09T01:14:37Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AdamSchiff between 2026-02-05T12:48:42Z and 2026-02-07T12:48:42Z\n",
      "SUCCESS\n",
      "Fetching tweets for AdamSchiff between 2026-02-07T02:36:01Z and 2026-02-09T02:36:01Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for AdamSchiff between 2026-02-06T09:13:19Z and 2026-02-08T09:13:19Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Saved → data/posts/AdamSchiff\\AdamSchiff_posts.json\n",
      "Fetching comments for 2019848459243909489 between 2026-02-06T18:59:34Z and 2026-02-08T18:59:34Z\n",
      "SUCCESS\n",
      "Saved → data/posts/AdamSchiff/2019848459243909489\\comments.json\n",
      "Collecting posts for MichaelBennet\n",
      "Fetching tweets for MichaelBennet between 2026-02-04T16:32:50Z and 2026-02-06T16:32:50Z\n",
      "SUCCESS\n",
      "Fetching tweets for MichaelBennet between 2026-02-07T20:46:33Z and 2026-02-09T20:46:33Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for MichaelBennet between 2026-02-07T08:00:33Z and 2026-02-09T08:00:33Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Fetching tweets for MichaelBennet between 2026-02-05T22:42:42Z and 2026-02-07T22:42:42Z\n",
      "SUCCESS\n",
      "Fetching tweets for MichaelBennet between 2026-02-07T19:31:06Z and 2026-02-09T19:31:06Z\n",
      "SUCCESS\n",
      "No new posts found - stopping early\n",
      "Saved → data/posts/MichaelBennet\\MichaelBennet_posts.json\n",
      "Already have 10 comments for 2018412264127123727\n",
      "Already have 10 comments for 2017383787156660270\n",
      "Already have 10 comments for 2017314146774085951\n",
      "Fetching comments for 2019453933026996522 between 2026-02-05T16:51:51Z and 2026-02-07T16:51:51Z\n",
      "SUCCESS\n",
      "Saved → data/posts/MichaelBennet/2019453933026996522\\comments.json\n",
      "Fetching comments for 2019229162331075000 between 2026-02-05T01:58:42Z and 2026-02-07T01:58:42Z\n",
      "SUCCESS\n",
      "Saved → data/posts/MichaelBennet/2019229162331075000\\comments.json\n",
      "Fetching comments for 2019850039607325017 between 2026-02-06T19:05:51Z and 2026-02-08T19:05:51Z\n",
      "SUCCESS\n",
      "Saved → data/posts/MichaelBennet/2019850039607325017\\comments.json\n"
     ]
    }
   ],
   "source": [
    "app = ToxicityApp()\n",
    "\n",
    "results = app.collect_politician_comments(\n",
    "    [\"SenTuberville\",\n",
    "     \"SenKatieBritt\",\n",
    "     \"lisamurkowski\",\n",
    "     \"SenDanSullivan\",\n",
    "     \"SenRubenGallego\",\n",
    "     \"SenMarkKelly\",\n",
    "     \"JohnBoozman\",\n",
    "     \"SenTomCotton\",\n",
    "     \"AlexPadilla4CA\",\n",
    "     \"AdamSchiff\",\n",
    "     \"MichaelBennet\"\n",
    "    ],\n",
    "    randomize_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd75d9-c320-4ee6-9640-a758eb7fc632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cc0e0-f1c1-45b3-a5f6-886fec9c897e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
